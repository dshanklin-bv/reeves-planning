# Two Primitives: Priors + Memory = Intelligence

## The Framework

All intelligence — biological, artificial, or otherwise — reduces to two primitives:

**Priors** — Compressed evolutionary history. Everything that was learned before this individual existed. In biology: DNA. In AI: model weights. The starting point.

**Memory** — Lived experience. Everything that happened to this specific individual in this specific environment. In biology: neural plasticity, immune memory, muscle memory. In AI: persistent state, logs, graphs, embeddings. The shaping force.

Intelligence = Priors + Memory.

Neither is sufficient alone. Both are required.

## Why Two, Not One

### Priors Without Memory: The Newborn

A human newborn has the full genome — 3.8 billion years of evolutionary priors. It can breathe, cry, suckle, and grip. But it can't walk, talk, reason, or survive independently. The priors provide the *capacity* for intelligence. Lived experience activates it.

An AI model fresh from training has enormous priors — the compressed patterns of the internet. It can generate fluent text, write code, analyze data. But it doesn't know *you*. Each conversation starts from zero. The priors are powerful but generic. Without memory, it's the same newborn every time.

### Memory Without Priors: Impossible

In biology, you can't have lived experience without a genome. There's no substrate to experience with. No nervous system to form memories in. No body to interact with the environment.

In AI, you can't have a persistent log that compounds into intelligence without a model capable of interpreting it. Raw data isn't memory. Memory is data + a system that can use it to behave differently next time.

### Priors + Memory: The Complete System

A 30-year-old human: genome (750 MB of evolutionary priors) + 30 years of lived experience shaping how those priors express. Same DNA template as every other human. Radically different person. Because the combination of priors + specific life experience produces something unique.

An AI system with persistent, compounding memory: model weights (terabytes of training priors) + months/years of accumulated interaction history, outcome data, and learned patterns for a specific environment. Same base model as every other instance. Radically different system. Because the memory shaped it.

## How Priors Work

### Biological Priors (DNA)
- **Encoding:** 3.2 billion base pairs, chemical storage
- **Created by:** 3.8 billion years of natural selection
- **Density:** ~50 MB of functional code encodes all human capability
- **Inheritance:** Sexual reproduction (recombination of two parents)
- **Update speed:** Generational (~25 years per cycle)
- **Selection pressure:** Death (binary, harsh, unforgiving)

### AI Priors (Model Weights)
- **Encoding:** Billions of floating-point parameters
- **Created by:** Gradient descent on training data
- **Density:** Terabytes encode impressive but generic capability
- **Inheritance:** Model versioning (GPT-3 → GPT-4 → GPT-5)
- **Update speed:** Months to years per training run
- **Selection pressure:** Loss function (continuous, mathematical, gentle)

## How Memory Works

### Biological Memory
- **Neural plasticity:** Synaptic connections strengthen/weaken with use
- **Immune memory:** T-cells and B-cells remember pathogens
- **Muscle memory:** Motor patterns encoded through repetition
- **Emotional memory:** Experiences tagged with emotional weight (amygdala)
- **Epigenetics:** Gene expression modified by environment (without changing DNA)
- **Key property:** Memory shapes how priors *express*, not the priors themselves

### AI Memory (What It Should Be)
- **Persistent log:** Every interaction, outcome, and observation recorded
- **Emotional weighting:** PEFM — urgency, insight, frustration, satisfaction, complexity, novelty (with decay)
- **Knowledge graph:** Relationships between entities, confidence-scored
- **Embeddings:** Semantic similarity across all stored content
- **Pattern consolidation:** Sleep cycles that extract heuristics from raw experience
- **Key property:** Memory shapes how the model's capabilities express for a specific environment

## The Insight

Intelligence isn't about having the best priors OR the most memory. It's about the *interaction* between them.

A larger genome doesn't make a smarter human. A longer life doesn't either. What matters is the richness of the interaction — how effectively lived experience shapes the expression of evolutionary priors.

Similarly, a larger model doesn't make a smarter AI. More stored data doesn't either. What matters is the richness of the loop — how effectively accumulated experience shapes the expression of trained capabilities for a specific environment.

The two-primitive framework points to exactly where current AI is weak: not in priors (model quality), but in memory (persistent, compounding, environment-specific lived experience). Fix the memory primitive, and the priors we already have become dramatically more powerful.
