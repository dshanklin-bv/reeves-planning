# 750 Megabytes

*The most devastating argument against the scaling hypothesis, hiding in plain sight.*

---

## The Numbers

Human DNA: 3.2 billion base pairs. 2 bits per pair. **750 megabytes.**

Functional coding regions (the parts that actually encode proteins): about 1.5% of the total. **~50 megabytes.**

A frontier large language model: hundreds of billions to trillions of parameters. **Hundreds of gigabytes to terabytes.**

The ratio is staggering. AI models are 1,000 to 10,000 times larger than the human genome in raw information storage. And yet:

## What 750 MB Specifies

It doesn't do any of this directly. It's a blueprint for building machinery that figures it out at runtime:

- An immune system that handles pathogens *that have never existed in the history of Earth* — not by encoding antibodies, but by encoding the machinery that generates antibodies on demand
- A brain capable of consciousness, mathematics, poetry, and deception — not by encoding thoughts, but by encoding the architecture that learns to think
- Coordination of 37 trillion cells across 200+ distinct types — not by micromanaging each cell, but by encoding developmental rules that self-organize
- Homeostasis across temperature, pH, oxygen, glucose, and thousands of other variables — not by hard-coding setpoints, but by encoding feedback loops that self-regulate
- Wound healing for injuries it has never encountered — not by cataloging wounds, but by encoding repair machinery that adapts to damage
- Development from a single fertilized cell into a complete organism — the spec *builds the system that builds itself*
- Adaptation to environments from the Arctic to the Sahara — same spec, different expression, shaped by whatever environment it lands in
- Offspring that inherit the full capability set — not the learned knowledge, just the spec. Every generation starts fresh and learns its own world.

All of this autonomously, without maintenance, for 70-80 years. The spec is 750 MB. Everything else is runtime.

## What Terabytes Do

- Encode frozen statistical patterns from a single training run
- Generate plausible text from those patterns
- Can't remember what you said yesterday
- Start fresh every conversation
- Require a data center to run

## Why the Gap Exists

The gap isn't about the quality of the information. Transformer weights are genuinely sophisticated — compressed statistical patterns across most of human written knowledge. That's remarkable.

The gap is about **how the information got there**.

This isn't because the genome is perfectly optimized. It isn't. Roughly 98% of human DNA is non-coding — transposons, pseudogenes, repetitive sequences, regulatory elements we're still mapping. Evolution doesn't clean up after itself. It's not an engineer. But the ~50 megabytes of functional coding regions that survived 3.8 billion years of selection pressure are the most reality-tested information in the known universe. Every one of those bytes earned its place because an organism carrying it lived long enough to reproduce and one without it didn't. The genome isn't clean. But it's been under continuous pressure longer than anything else on Earth, and the signal it carries — buried in the noise — encodes machinery that no engineered system has come close to matching.

AI model weights were placed by gradient descent on a static dataset. A mathematical optimization found the parameters that minimize prediction error across internet text. That's a powerful process, but it ran once, against a fixed corpus, optimizing for a single objective (next token prediction). It was never tested against reality. No weight earned its place by surviving anything.

That's why 750 MB beats terabytes. Density isn't about compression algorithms. It's about selection pressure. Information forged against reality for billions of years is denser than information optimized against a loss function for months.

## The Density Argument

Think about what "information density" really means.

A fear-of-heights response occupies a tiny neural circuit — maybe a few thousand neurons. But it handles *every height-related scenario that will ever exist*: cliffs, rooftops, glass floors, VR, airplanes, trees, ladders, bridges, balconies. No enumeration required. One heuristic, infinite coverage.

A deterministic height-safety AI would need to encode every dangerous elevation scenario explicitly. Each new scenario = more parameters. The storage grows linearly with the number of cases. It will never finish because the case space is infinite.

Heuristics compress infinitely better than lookup tables because they don't store answers. They store *strategies for finding answers*. That's why a genome can be small and cover everything. It doesn't know the answers. It knows how to build systems that find answers.

## The Implication for AI

The scaling hypothesis says: intelligence emerges from scale. Make the model bigger, train on more data, and capabilities will continue to improve.

The 750 MB argument says: the most intelligent system we know of uses 1,000x less information than current AI models and handles incomparably more scenarios. Scale isn't the bottleneck. Density is. And density comes from selection pressure, not from bigger training runs.

This doesn't mean scaling is useless. More capable base models are genuinely better substrates. But substrate improvement has diminishing returns when the real bottleneck is elsewhere.

The real bottleneck: current AI information was never tested against reality. It was tested against a loss function. To achieve biological information density, AI needs biological-grade selection pressure — continuous, consequential, and compounding over time.

750 megabytes is proof that this is possible. It's proof that intelligence doesn't require enormous storage. It requires enormous *pressure* applied over enormous *time* to information that *persists*.

That's the roadmap. Not bigger models. Better loops. Run longer. Against reality.
