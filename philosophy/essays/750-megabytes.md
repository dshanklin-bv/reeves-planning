# 750 Megabytes

*The most devastating argument against the scaling hypothesis, hiding in plain sight.*

---

## The Numbers

Human DNA: 3.2 billion base pairs. 2 bits per pair. **750 megabytes.**

Functional coding regions (the parts that actually encode proteins): about 1.5% of the total. **~50 megabytes.**

A frontier large language model: hundreds of billions to trillions of parameters. **Hundreds of gigabytes to terabytes.**

The ratio is staggering. AI models are 1,000 to 10,000 times larger than the human genome in raw information storage. And yet:

## What 750 MB Does

- Builds an immune system that handles pathogens *that have never existed in the history of Earth*
- Constructs a brain capable of consciousness, mathematics, poetry, and deception
- Coordinates 37 trillion cells across 200+ distinct types
- Maintains homeostasis across temperature, pH, oxygen, glucose, and thousands of other variables simultaneously
- Heals wounds it has never encountered before
- Develops from a single fertilized cell into a complete organism
- Adapts to environments from the Arctic to the Sahara
- Produces offspring that inherit the full capability set
- Does all of this autonomously, without maintenance, for 70-80 years

## What Terabytes Do

- Generate plausible text
- Can't remember what you said yesterday
- Start fresh every conversation
- Require a data center to run

## Why the Gap Exists

The gap isn't about the quality of the information. Transformer weights are genuinely sophisticated — compressed statistical patterns across most of human written knowledge. That's remarkable.

The gap is about **how the information got there**.

Every byte of DNA was placed by survival. An organism with that byte lived long enough to reproduce. An organism without it didn't. Over 3.8 billion years, across trillions of trillions of organisms, this filter ran continuously. What remains is the most aggressively compressed, most reality-tested information in the known universe.

AI model weights were placed by gradient descent on a static dataset. A mathematical optimization found the parameters that minimize prediction error across internet text. That's a powerful process, but it ran once, against a fixed corpus, optimizing for a single objective (next token prediction). It was never tested against reality. No weight earned its place by surviving anything.

That's why 750 MB beats terabytes. Density isn't about compression algorithms. It's about selection pressure. Information forged against reality for billions of years is denser than information optimized against a loss function for months.

## The Density Argument

Think about what "information density" really means.

A fear-of-heights response occupies a tiny neural circuit — maybe a few thousand neurons. But it handles *every height-related scenario that will ever exist*: cliffs, rooftops, glass floors, VR, airplanes, trees, ladders, bridges, balconies. No enumeration required. One heuristic, infinite coverage.

A deterministic height-safety AI would need to encode every dangerous elevation scenario explicitly. Each new scenario = more parameters. The storage grows linearly with the number of cases. It will never finish because the case space is infinite.

Heuristics compress infinitely better than lookup tables because they don't store answers. They store *strategies for finding answers*. That's why a genome can be small and cover everything. It doesn't know the answers. It knows how to build systems that find answers.

## The Implication for AI

The scaling hypothesis says: intelligence emerges from scale. Make the model bigger, train on more data, and capabilities will continue to improve.

The 750 MB argument says: the most intelligent system we know of uses 1,000x less information than current AI models and handles incomparably more scenarios. Scale isn't the bottleneck. Density is. And density comes from selection pressure, not from bigger training runs.

This doesn't mean scaling is useless. More capable base models are genuinely better substrates. But substrate improvement has diminishing returns when the real bottleneck is elsewhere.

The real bottleneck: current AI information was never tested against reality. It was tested against a loss function. To achieve biological information density, AI needs biological-grade selection pressure — continuous, consequential, and compounding over time.

750 megabytes is proof that this is possible. It's proof that intelligence doesn't require enormous storage. It requires enormous *pressure* applied over enormous *time* to information that *persists*.

That's the roadmap. Not bigger models. Better loops. Run longer. Against reality.
